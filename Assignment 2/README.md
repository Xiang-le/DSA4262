# Assignment 2
### Motivation
A key challenge in AI for mental health is to quantify subjective and “invisible” experiences. While clinical settings provide structured data, much of our modern mental health discourse happens in informal digital spaces. In this second Individual Assignment, you will build a predictive model to detect stress signals in social media text using the Dreaddit dataset. This requires you to translate raw, noisy human expression into a reliable clinical signal. The dataset contains informal and indirect language and highlights how mental health is influenced by community dynamics.

### Description
You will build a binary classification model to distinguish between “Stressed” and “Non-stressed” posts across multiple Reddit communities. The Dreadit dataset comes with a pre-defined test dataset. Your key performance metric will be the F1-score on this test dataset. However, a good F1-score will just get you a pass. This course is about sense-making, so your job is to go beyond this. There are many analyses you can add to your project, for example: 1) analysing which subreddit is easier to predict and speculate on the reason, 2) pick out a few cases where your model confidently predicted the wrong label, and discuss why, 3) pick out specific features or phrases that predict stress, and many more. You don’t have to do every analysis possible but be creative and add around 3 additional analyses. These should feed into your final conclusion: how your model may be used in a real-world setting (for example, in Singapore, for let’s talk analysis), and what limitations or risks these usages may have. A good predictive performance can be compensated by an interesting deep dive and a thoughtful consideration of your model’s deployment.

You can take any modelling approach you want, including BERT, FastText, traditional ML with LIWC and other features, LLMs, etc. The dataset actually includes many pre-computed features such as LIWC and sentiment, you can use these. However, you are free to compute additional features. You should include your rationale for your model choice. Or, you could compare several models and discuss any differences in performance. You are free to use any AI tools you want, especially if they help you write cleaner and more interpretable code. Try to limit your discussion with other students to technical discussions only. The choice of deeper dive analyses should be yours alone.

### Deliverable
Students have to develop their models in a Jupyter Notebook. You should submit a PDF of your notebook to Canvas, which should also include details as Markdown block on data processing, model architecture (choice and motivation), and performance analysis (e.g. error analysis of failure modes). A link to your GitHub repository with reproducible code should be included. If we are not able to run your notebooks, you will get a low score. Start your notebooks with some data visualisation and finish your notebook with a concluding statement on your findings.

### Assessment
The assessment will not just focus on “leaderboard” ranking. Rigorous methodology and clinical sense-making will also be assessed. A thoughtful explanation and analysis of the model can compensate for a lower accuracy score.
